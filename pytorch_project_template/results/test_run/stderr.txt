2019-06-03 12:24:32,438 - src.vegab - INFO - Calling function train
2019-06-03 12:24:57,702 - root - INFO - Transposing
2019-06-03 12:24:57,822 - root - INFO - x_train shape:(1280, 3, 32, 32)
2019-06-03 12:24:57,822 - root - INFO - 1280train samples
2019-06-03 12:24:57,822 - root - INFO - 1280test samples
2019-06-03 12:28:13,648 - src.vegab - INFO - Calling function train
2019-06-03 12:28:13,918 - root - INFO - Transposing
2019-06-03 12:28:14,017 - root - INFO - x_train shape:(1280, 3, 32, 32)
2019-06-03 12:28:14,017 - root - INFO - 1280train samples
2019-06-03 12:28:14,017 - root - INFO - 1280test samples
2019-06-03 12:32:16,998 - src.vegab - INFO - Calling function train
2019-06-03 12:32:17,282 - root - INFO - Transposing
2019-06-03 12:32:17,402 - root - INFO - x_train shape:(1280, 3, 32, 32)
2019-06-03 12:32:17,402 - root - INFO - 1280train samples
2019-06-03 12:32:17,402 - root - INFO - 1280test samples
2019-06-03 12:32:17,432 - src.training_loop - INFO - Removing results/test_run/history.pkl and results/test_run/history.csv
2019-06-03 12:32:17,483 - src.training_loop - INFO - Saving model from beginning
2019-06-03 12:32:17,485 - src.callbacks - INFO - Fix learning rate to 0.01
/Users/jastrs01/Dropbox/Projekty/toolkit/vanilla_pytorch_project_template/src/models/simple_cnn.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.log_softmax(x)  # TODO: Shoulnt be linear
2019-06-03 12:32:20,849 - src.training_loop - INFO - epoch=0	loss=5.873900461196899	time=3.3637754120000007	acc=9.53125	val_loss=5.868473100662231	val_acc=8.671875	
2019-06-03 12:32:20,849 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:32:20,855 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:32:20,857 - src.training_loop - INFO - Saving model from epoch 0
2019-06-03 12:32:20,864 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:32:20,866 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:32:20,867 - src.callbacks - INFO - Fix learning rate to 0.01
2019-06-03 12:32:24,807 - src.training_loop - INFO - epoch=1	loss=5.86667275428772	time=3.9399294020000006	acc=9.296875	val_loss=5.865404033660889	val_acc=10.9375	
2019-06-03 12:32:24,807 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:32:24,809 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:32:24,811 - src.training_loop - INFO - Saving model from epoch 1
2019-06-03 12:32:24,813 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:32:24,813 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:32:24,814 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:32:28,942 - src.training_loop - INFO - epoch=2	loss=5.865535259246826	time=4.127890742	acc=9.6875	val_loss=5.865256690979004	val_acc=11.015625	
2019-06-03 12:32:28,942 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:32:28,944 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:32:28,946 - src.training_loop - INFO - Saving model from epoch 2
2019-06-03 12:32:28,948 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:32:28,948 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:32:28,948 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:32:33,190 - src.training_loop - INFO - epoch=3	loss=5.865106534957886	time=4.240864624	acc=11.09375	val_loss=5.865125465393066	val_acc=11.015625	
2019-06-03 12:32:33,190 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:32:33,193 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:32:33,193 - src.training_loop - INFO - Saving model from epoch 3
2019-06-03 12:32:33,196 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:32:33,196 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:32:33,196 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:32:38,589 - src.training_loop - INFO - epoch=4	loss=5.8649582862854	time=5.392061899999998	acc=11.09375	val_loss=5.865006160736084	val_acc=10.9375	
2019-06-03 12:32:38,589 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:32:38,592 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:32:38,593 - src.training_loop - INFO - Saving model from epoch 4
2019-06-03 12:32:38,595 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:32:38,596 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:32:38,596 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:32:42,742 - src.training_loop - INFO - epoch=5	loss=5.864862775802612	time=4.145590430999999	acc=11.640625	val_loss=5.8648944854736325	val_acc=11.40625	
2019-06-03 12:32:42,742 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:32:42,745 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:32:42,747 - src.training_loop - INFO - Saving model from epoch 5
2019-06-03 12:32:42,750 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:32:42,750 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:32:42,750 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:32:47,686 - src.training_loop - INFO - epoch=6	loss=5.8648529052734375	time=4.935551260000004	acc=11.40625	val_loss=5.864791297912598	val_acc=11.796875	
2019-06-03 12:32:47,686 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:32:47,699 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:32:47,707 - src.training_loop - INFO - Saving model from epoch 6
2019-06-03 12:32:47,712 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:32:47,716 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:32:47,716 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:32:51,803 - src.training_loop - INFO - epoch=7	loss=5.864695882797241	time=4.086248273999999	acc=11.171875	val_loss=5.864696979522705	val_acc=12.1875	
2019-06-03 12:32:51,803 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:32:51,806 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:32:51,808 - src.training_loop - INFO - Saving model from epoch 7
2019-06-03 12:32:51,810 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:32:51,811 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:32:51,811 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:32:55,888 - src.training_loop - INFO - epoch=8	loss=5.864626407623291	time=4.076887494999994	acc=10.3125	val_loss=5.864609575271606	val_acc=12.265625	
2019-06-03 12:32:55,888 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:32:55,891 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:32:55,893 - src.training_loop - INFO - Saving model from epoch 8
2019-06-03 12:32:55,895 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:32:55,895 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:32:55,895 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:32:59,854 - src.training_loop - INFO - epoch=9	loss=5.864520072937012	time=3.9582942880000047	acc=11.40625	val_loss=5.864530467987061	val_acc=12.578125	
2019-06-03 12:32:59,854 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:32:59,857 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:32:59,859 - src.training_loop - INFO - Saving model from epoch 9
2019-06-03 12:32:59,861 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:32:59,862 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:32:59,862 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:33:03,725 - src.training_loop - INFO - epoch=10	loss=5.864494895935058	time=3.8630491719999966	acc=11.640625	val_loss=5.864458179473877	val_acc=12.96875	
2019-06-03 12:33:03,725 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:33:03,728 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:33:03,730 - src.training_loop - INFO - Saving model from epoch 10
2019-06-03 12:33:03,731 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:33:03,732 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:33:03,732 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:33:07,629 - src.training_loop - INFO - epoch=11	loss=5.864364862442017	time=3.896194150999996	acc=12.734375	val_loss=5.864388942718506	val_acc=12.734375	
2019-06-03 12:33:07,629 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:33:07,632 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:33:07,632 - src.training_loop - INFO - Saving model from epoch 11
2019-06-03 12:33:07,634 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:33:07,635 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:33:07,635 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:33:11,535 - src.training_loop - INFO - epoch=12	loss=5.8642748355865475	time=3.899469846999999	acc=11.5625	val_loss=5.864325380325317	val_acc=12.734375	
2019-06-03 12:33:11,535 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:33:11,538 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:33:11,538 - src.training_loop - INFO - Saving model from epoch 12
2019-06-03 12:33:11,540 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:33:11,540 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:33:11,541 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:33:15,442 - src.training_loop - INFO - epoch=13	loss=5.864325523376465	time=3.9008426259999993	acc=11.875	val_loss=5.864267015457154	val_acc=12.8125	
2019-06-03 12:33:15,442 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:33:15,446 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:33:15,446 - src.training_loop - INFO - Saving model from epoch 13
2019-06-03 12:33:15,448 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:33:15,449 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:33:15,449 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:33:19,301 - src.training_loop - INFO - epoch=14	loss=5.864067697525025	time=3.8517646509999963	acc=13.671875	val_loss=5.864211559295654	val_acc=12.890625	
2019-06-03 12:33:19,302 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:33:19,304 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:33:19,305 - src.training_loop - INFO - Saving model from epoch 14
2019-06-03 12:33:19,307 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:33:19,307 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:33:19,308 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:33:23,006 - src.training_loop - INFO - epoch=15	loss=5.864199924468994	time=3.697764058000004	acc=13.28125	val_loss=5.864160013198853	val_acc=13.125	
2019-06-03 12:33:23,006 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:33:23,010 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:33:23,012 - src.training_loop - INFO - Saving model from epoch 15
2019-06-03 12:33:23,014 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:33:23,014 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:33:23,015 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:33:26,559 - src.training_loop - INFO - epoch=16	loss=5.864033889770508	time=3.544529194000006	acc=12.578125	val_loss=5.864107704162597	val_acc=13.359375	
2019-06-03 12:33:26,560 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:33:26,562 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:33:26,563 - src.training_loop - INFO - Saving model from epoch 16
2019-06-03 12:33:26,565 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:33:26,565 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:33:26,565 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:33:30,193 - src.training_loop - INFO - epoch=17	loss=5.863847494125366	time=3.6272349309999896	acc=13.515625	val_loss=5.864058637619019	val_acc=13.125	
2019-06-03 12:33:30,193 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:33:30,196 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:33:30,197 - src.training_loop - INFO - Saving model from epoch 17
2019-06-03 12:33:30,198 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:33:30,199 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:33:30,199 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:40:21,842 - src.vegab - INFO - Calling function train
2019-06-03 12:40:22,128 - root - INFO - Transposing
2019-06-03 12:40:22,248 - root - INFO - x_train shape:(1280, 3, 32, 32)
2019-06-03 12:40:22,248 - root - INFO - 1280train samples
2019-06-03 12:40:22,248 - root - INFO - 1280test samples
2019-06-03 12:40:22,289 - src.training_loop - INFO - Removing results/test_run/history.pkl and results/test_run/history.csv
2019-06-03 12:40:22,342 - src.training_loop - INFO - Saving model from beginning
2019-06-03 12:40:22,344 - src.callbacks - INFO - Fix learning rate to 0.01
/Users/jastrs01/Dropbox/Projekty/toolkit/vanilla_pytorch_project_template/src/models/simple_cnn.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.log_softmax(x)  # TODO: Shoulnt be linear
2019-06-03 12:45:29,853 - src.vegab - INFO - Calling function train
2019-06-03 12:45:30,089 - root - INFO - Transposing
2019-06-03 12:45:30,185 - root - INFO - x_train shape:(1280, 3, 32, 32)
2019-06-03 12:45:30,185 - root - INFO - 1280train samples
2019-06-03 12:45:30,185 - root - INFO - 1280test samples
2019-06-03 12:45:30,220 - src.training_loop - INFO - Removing results/test_run/history.pkl and results/test_run/history.csv
2019-06-03 12:45:30,279 - src.training_loop - INFO - Saving model from beginning
2019-06-03 12:45:30,284 - src.callbacks - INFO - Fix learning rate to 0.01
/Users/jastrs01/Dropbox/Projekty/toolkit/vanilla_pytorch_project_template/src/models/simple_cnn.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.log_softmax(x)  # TODO: Shoulnt be linear
2019-06-03 12:45:35,049 - src.training_loop - INFO - epoch=0	loss=5.868478727340698	time=4.764146728000001	acc=9.21875	val_loss=5.86574387550354	val_acc=10.15625	
2019-06-03 12:45:35,049 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:45:35,052 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:45:35,053 - src.training_loop - INFO - Saving model from epoch 0
2019-06-03 12:45:35,055 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:45:35,056 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:45:35,056 - src.callbacks - INFO - Fix learning rate to 0.01
2019-06-03 12:45:38,678 - src.training_loop - INFO - epoch=1	loss=5.864586734771729	time=3.621938743000001	acc=10.078125	val_loss=5.864084100723266	val_acc=11.5625	
2019-06-03 12:45:38,678 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:45:38,681 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:45:38,685 - src.training_loop - INFO - Saving model from epoch 1
2019-06-03 12:45:38,687 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:45:38,687 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:45:38,688 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:45:42,216 - src.training_loop - INFO - epoch=2	loss=5.863698196411133	time=3.5280836079999993	acc=10.9375	val_loss=5.863994932174682	val_acc=11.71875	
2019-06-03 12:45:42,216 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:45:42,218 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:45:42,220 - src.training_loop - INFO - Saving model from epoch 2
2019-06-03 12:45:42,221 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:45:42,222 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:45:42,222 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:45:45,425 - src.training_loop - INFO - epoch=3	loss=5.863855171203613	time=3.202579725000003	acc=10.3125	val_loss=5.863914585113525	val_acc=11.796875	
2019-06-03 12:45:45,425 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:45:45,427 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:45:45,429 - src.training_loop - INFO - Saving model from epoch 3
2019-06-03 12:45:45,430 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:45:45,431 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:45:45,431 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:45:48,394 - src.training_loop - INFO - epoch=4	loss=5.863766479492187	time=2.96348274	acc=11.484375	val_loss=5.863839483261108	val_acc=12.109375	
2019-06-03 12:45:48,394 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:45:48,396 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:45:48,398 - src.training_loop - INFO - Saving model from epoch 4
2019-06-03 12:45:48,399 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:45:48,399 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:45:48,400 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:45:51,459 - src.training_loop - INFO - epoch=5	loss=5.86361985206604	time=3.0590141939999995	acc=12.578125	val_loss=5.863772916793823	val_acc=12.109375	
2019-06-03 12:45:51,459 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:45:51,463 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:45:51,464 - src.training_loop - INFO - Saving model from epoch 5
2019-06-03 12:45:51,466 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:45:51,466 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:45:51,466 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:45:54,603 - src.training_loop - INFO - epoch=6	loss=5.863342237472534	time=3.134069441000001	acc=12.421875	val_loss=5.863711500167847	val_acc=12.421875	
2019-06-03 12:45:54,603 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:45:54,606 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:45:54,607 - src.training_loop - INFO - Saving model from epoch 6
2019-06-03 12:45:54,608 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:45:54,609 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:45:54,609 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:45:57,797 - src.training_loop - INFO - epoch=7	loss=5.863440036773682	time=3.187730658000003	acc=11.796875	val_loss=5.863653659820557	val_acc=12.734375	
2019-06-03 12:45:57,797 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:45:57,799 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:45:57,801 - src.training_loop - INFO - Saving model from epoch 7
2019-06-03 12:45:57,802 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:45:57,803 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:45:57,803 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:00,935 - src.training_loop - INFO - epoch=8	loss=5.8635327339172365	time=3.131395150999996	acc=11.171875	val_loss=5.863601016998291	val_acc=12.96875	
2019-06-03 12:46:00,935 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:00,937 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:00,939 - src.training_loop - INFO - Saving model from epoch 8
2019-06-03 12:46:00,940 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:00,941 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:00,941 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:03,987 - src.training_loop - INFO - epoch=9	loss=5.8632872104644775	time=3.045526298999995	acc=12.578125	val_loss=5.863552761077881	val_acc=13.125	
2019-06-03 12:46:03,987 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:03,991 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:03,994 - src.training_loop - INFO - Saving model from epoch 9
2019-06-03 12:46:03,995 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:03,996 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:03,996 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:07,133 - src.training_loop - INFO - epoch=10	loss=5.86327862739563	time=3.1363581899999957	acc=11.953125	val_loss=5.863507223129273	val_acc=13.125	
2019-06-03 12:46:07,133 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:07,135 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:07,135 - src.training_loop - INFO - Saving model from epoch 10
2019-06-03 12:46:07,137 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:07,137 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:07,137 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:10,611 - src.training_loop - INFO - epoch=11	loss=5.863283157348633	time=3.4737501680000022	acc=11.796875	val_loss=5.863464975357056	val_acc=13.28125	
2019-06-03 12:46:10,612 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:10,615 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:10,616 - src.training_loop - INFO - Saving model from epoch 11
2019-06-03 12:46:10,618 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:10,619 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:10,619 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:14,665 - src.training_loop - INFO - epoch=12	loss=5.863224840164184	time=4.045464068000001	acc=15.3125	val_loss=5.863424062728882	val_acc=13.203125	
2019-06-03 12:46:14,665 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:14,668 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:14,669 - src.training_loop - INFO - Saving model from epoch 12
2019-06-03 12:46:14,671 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:14,672 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:14,672 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:18,451 - src.training_loop - INFO - epoch=13	loss=5.863150978088379	time=3.7785924960000017	acc=12.96875	val_loss=5.863385105133057	val_acc=13.515625	
2019-06-03 12:46:18,451 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:18,454 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:18,455 - src.training_loop - INFO - Saving model from epoch 13
2019-06-03 12:46:18,457 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:18,458 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:18,458 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:22,327 - src.training_loop - INFO - epoch=14	loss=5.863137674331665	time=3.8692256249999986	acc=13.4375	val_loss=5.863349676132202	val_acc=13.828125	
2019-06-03 12:46:22,328 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:22,330 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:22,332 - src.training_loop - INFO - Saving model from epoch 14
2019-06-03 12:46:22,335 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:22,335 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:22,336 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:26,163 - src.training_loop - INFO - epoch=15	loss=5.863131713867188	time=3.8271467659999985	acc=13.046875	val_loss=5.863316822052002	val_acc=13.4375	
2019-06-03 12:46:26,163 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:26,167 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:26,167 - src.training_loop - INFO - Saving model from epoch 15
2019-06-03 12:46:26,170 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:26,170 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:26,170 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:29,882 - src.training_loop - INFO - epoch=16	loss=5.863122177124024	time=3.7108636880000034	acc=13.046875	val_loss=5.863285493850708	val_acc=13.828125	
2019-06-03 12:46:29,882 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:29,884 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:29,885 - src.training_loop - INFO - Saving model from epoch 16
2019-06-03 12:46:29,886 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:29,886 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:29,887 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:33,655 - src.training_loop - INFO - epoch=17	loss=5.862924861907959	time=3.7655399210000056	acc=13.59375	val_loss=5.863256025314331	val_acc=13.828125	
2019-06-03 12:46:33,655 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:33,659 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:33,660 - src.training_loop - INFO - Saving model from epoch 17
2019-06-03 12:46:33,662 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:33,662 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:33,662 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:37,333 - src.training_loop - INFO - epoch=18	loss=5.862975311279297	time=3.669978284999999	acc=15.234375	val_loss=5.863227224349975	val_acc=14.21875	
2019-06-03 12:46:37,333 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:37,336 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:37,337 - src.training_loop - INFO - Saving model from epoch 18
2019-06-03 12:46:37,339 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:37,340 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:37,340 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 12:46:41,460 - src.training_loop - INFO - epoch=19	loss=5.8628755569458	time=4.12007118599999	acc=14.375	val_loss=5.8631998062133786	val_acc=14.21875	
2019-06-03 12:46:41,460 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 12:46:41,463 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 12:46:41,463 - src.training_loop - INFO - Saving model from epoch 19
2019-06-03 12:46:41,464 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 12:46:41,465 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 12:46:41,465 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:03:37,949 - src.vegab - INFO - Calling function train
2019-06-03 13:03:38,264 - root - INFO - Transposing
2019-06-03 13:03:38,379 - root - INFO - x_train shape:(1280, 3, 32, 32)
2019-06-03 13:03:38,380 - root - INFO - 1280train samples
2019-06-03 13:03:38,380 - root - INFO - 1280test samples
2019-06-03 13:03:38,426 - src.training_loop - INFO - Removing results/test_run/history.pkl and results/test_run/history.csv
2019-06-03 13:03:38,478 - src.training_loop - INFO - Saving model from beginning
2019-06-03 13:03:38,481 - src.callbacks - INFO - Fix learning rate to 0.01
/Users/jastrs01/Dropbox/Projekty/toolkit/vanilla_pytorch_project_template/src/models/simple_cnn.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.log_softmax(x)  # TODO: Shoulnt be linear
2019-06-03 13:03:41,426 - src.training_loop - INFO - epoch=0	loss=5.87621169090271	time=2.9448343149999996	acc=7.5	val_loss=5.866535186767578	val_acc=9.453125	
2019-06-03 13:03:41,426 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:03:41,432 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:03:41,433 - src.training_loop - INFO - Saving model from epoch 0
2019-06-03 13:03:41,435 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:03:41,436 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:03:41,436 - src.callbacks - INFO - Fix learning rate to 0.01
2019-06-03 13:03:44,701 - src.training_loop - INFO - epoch=1	loss=5.8652183532714846	time=3.2643110569999987	acc=10.859375	val_loss=5.8645734786987305	val_acc=11.328125	
2019-06-03 13:03:44,701 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:03:44,703 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:03:44,704 - src.training_loop - INFO - Saving model from epoch 1
2019-06-03 13:03:44,706 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:03:44,706 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:03:44,706 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:03:47,693 - src.training_loop - INFO - epoch=2	loss=5.8643927574157715	time=2.9862836789999996	acc=12.03125	val_loss=5.864495706558228	val_acc=11.71875	
2019-06-03 13:03:47,693 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:03:47,695 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:03:47,696 - src.training_loop - INFO - Saving model from epoch 2
2019-06-03 13:03:47,698 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:03:47,698 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:03:47,698 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:03:51,080 - src.training_loop - INFO - epoch=3	loss=5.864106369018555	time=3.3816195009999994	acc=13.671875	val_loss=5.8644250392913815	val_acc=12.03125	
2019-06-03 13:03:51,080 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:03:51,083 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:03:51,085 - src.training_loop - INFO - Saving model from epoch 3
2019-06-03 13:03:51,087 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:03:51,087 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:03:51,087 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:03:54,304 - src.training_loop - INFO - epoch=4	loss=5.8642387866973875	time=3.216055836999999	acc=13.359375	val_loss=5.864360761642456	val_acc=12.1875	
2019-06-03 13:03:54,304 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:03:54,306 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:03:54,308 - src.training_loop - INFO - Saving model from epoch 4
2019-06-03 13:03:54,309 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:03:54,309 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:03:54,309 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:03:57,459 - src.training_loop - INFO - epoch=5	loss=5.8639837265014645	time=3.1490952870000015	acc=13.359375	val_loss=5.864302396774292	val_acc=12.1875	
2019-06-03 13:03:57,459 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:03:57,463 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:03:57,463 - src.training_loop - INFO - Saving model from epoch 5
2019-06-03 13:03:57,465 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:03:57,465 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:03:57,465 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:04:00,675 - src.training_loop - INFO - epoch=6	loss=5.863964986801148	time=3.210038619999999	acc=12.5	val_loss=5.864248466491699	val_acc=12.421875	
2019-06-03 13:04:00,675 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:04:00,678 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:04:00,679 - src.training_loop - INFO - Saving model from epoch 6
2019-06-03 13:04:00,681 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:04:00,681 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:04:00,681 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:04:04,161 - src.training_loop - INFO - epoch=7	loss=5.8639349937438965	time=3.479586756	acc=12.734375	val_loss=5.864197254180908	val_acc=12.578125	
2019-06-03 13:04:04,161 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:04:04,164 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:04:04,166 - src.training_loop - INFO - Saving model from epoch 7
2019-06-03 13:04:04,168 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:04:04,168 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:04:04,168 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:04:07,774 - src.training_loop - INFO - epoch=8	loss=5.863711833953857	time=3.6049224770000023	acc=12.734375	val_loss=5.8641486167907715	val_acc=12.734375	
2019-06-03 13:04:07,774 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:04:07,776 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:04:07,778 - src.training_loop - INFO - Saving model from epoch 8
2019-06-03 13:04:07,779 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:04:07,780 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:04:07,780 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:04:11,354 - src.training_loop - INFO - epoch=9	loss=5.863914060592651	time=3.5738869229999963	acc=14.53125	val_loss=5.864100646972656	val_acc=12.96875	
2019-06-03 13:04:11,354 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:04:11,357 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:04:11,359 - src.training_loop - INFO - Saving model from epoch 9
2019-06-03 13:04:11,360 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:04:11,361 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:04:11,361 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:04:14,916 - src.training_loop - INFO - epoch=10	loss=5.863699436187744	time=3.5542812469999987	acc=13.203125	val_loss=5.8640556812286375	val_acc=13.125	
2019-06-03 13:04:14,916 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:04:14,918 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:04:14,919 - src.training_loop - INFO - Saving model from epoch 10
2019-06-03 13:04:14,920 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:04:14,921 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:04:14,921 - src.callbacks - INFO - Fix learning rate to 0.001
2019-06-03 13:59:05,864 - src.vegab - INFO - Calling function train
2019-06-03 13:59:06,288 - root - INFO - Transposing
2019-06-03 13:59:06,377 - root - INFO - x_train shape:(1280, 3, 32, 32)
2019-06-03 13:59:06,377 - root - INFO - 1280train samples
2019-06-03 13:59:06,377 - root - INFO - 1280test samples
2019-06-03 13:59:06,414 - src.training_loop - INFO - Removing results/test_run/history.pkl and results/test_run/history.csv
2019-06-03 13:59:54,304 - src.vegab - INFO - Calling function train
2019-06-03 13:59:54,501 - root - INFO - Transposing
2019-06-03 13:59:54,598 - root - INFO - x_train shape:(1280, 3, 32, 32)
2019-06-03 13:59:54,599 - root - INFO - 1280train samples
2019-06-03 13:59:54,599 - root - INFO - 1280test samples
2019-06-03 13:59:54,645 - src.training_loop - INFO - Removing results/test_run/history.pkl and results/test_run/history.csv
2019-06-03 13:59:54,744 - src.training_loop - INFO - Saving model from beginning
2019-06-03 13:59:54,747 - src.callbacks - INFO - Fix learning rate to 0.01
/Users/jastrs01/Dropbox/Projekty/toolkit/vanilla_pytorch_project_template/src/models/simple_cnn.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.log_softmax(x)  # TODO: Shoulnt be linear
2019-06-03 13:59:57,639 - src.training_loop - INFO - epoch=0	loss=5.879959535598755	time=2.8918480470000003	acc=9.0625	val_loss=5.8705347061157225	val_acc=8.515625	
2019-06-03 13:59:57,639 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 13:59:57,644 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 13:59:57,645 - src.training_loop - INFO - Saving model from epoch 0
2019-06-03 13:59:57,648 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 13:59:57,649 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 13:59:57,649 - src.callbacks - INFO - Fix learning rate to 0.01
2019-06-03 14:00:00,490 - src.training_loop - INFO - epoch=1	loss=5.867760419845581	time=2.8409758799999993	acc=9.453125	val_loss=5.865718746185303	val_acc=10.0	
2019-06-03 14:00:00,490 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 14:00:00,492 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 14:00:00,493 - src.training_loop - INFO - Saving model from epoch 1
2019-06-03 14:00:00,494 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 14:00:00,495 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 14:00:00,534 - src.vegab - INFO - Finished train
2019-06-03 14:00:21,379 - src.vegab - INFO - Calling function train
2019-06-03 14:00:21,585 - root - INFO - Transposing
2019-06-03 14:00:21,669 - root - INFO - x_train shape:(1280, 3, 32, 32)
2019-06-03 14:00:21,670 - root - INFO - 1280train samples
2019-06-03 14:00:21,670 - root - INFO - 1280test samples
2019-06-03 14:00:21,698 - src.training_loop - INFO - Removing results/test_run/history.pkl and results/test_run/history.csv
2019-06-03 14:00:21,764 - src.training_loop - INFO - Saving model from beginning
2019-06-03 14:00:21,766 - src.callbacks - INFO - Fix learning rate to 0.01
/Users/jastrs01/Dropbox/Projekty/toolkit/vanilla_pytorch_project_template/src/models/simple_cnn.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.log_softmax(x)  # TODO: Shoulnt be linear
2019-06-03 14:00:24,589 - src.training_loop - INFO - epoch=0	loss=5.873429012298584	time=2.823490671	acc=8.046875	val_loss=5.867412757873535	val_acc=8.75	
2019-06-03 14:00:24,589 - src.training_loop - INFO - Saving history to results/test_run/history.csv
2019-06-03 14:00:24,592 - src.callbacks - INFO - Saving history to results/test_run/history.pkl
2019-06-03 14:00:24,593 - src.training_loop - INFO - Saving model from epoch 0
2019-06-03 14:00:24,595 - src.training_loop - INFO - Saving loop_state.pkl
2019-06-03 14:00:24,596 - src.training_loop - INFO - Saved loop_state.pkl
2019-06-03 14:00:24,634 - src.vegab - INFO - Finished train
2019-06-03 14:05:27,564 - src.vegab - INFO - Calling function train
2019-06-03 14:05:27,770 - root - INFO - Transposing
2019-06-03 14:05:27,858 - root - INFO - x_train shape:(1280, 3, 32, 32)
2019-06-03 14:05:27,858 - root - INFO - 1280train samples
2019-06-03 14:05:27,858 - root - INFO - 1280test samples
2019-06-03 14:05:27,896 - src.training_loop - INFO - Removing results/test_run/history.pkl and results/test_run/history.csv
2019-06-03 14:05:27,941 - src.callbacks - INFO - Finished training. Exiting. Remove FINISHED file if you want to train anyways.
